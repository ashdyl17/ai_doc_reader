# AI Document Reader & QA Bot

This is an interactive Streamlit web application that allows users to upload PDF documents, extract text, generate AI summaries, store document content in a FAISS vector store for semantic search, and answer questions based on the document content. The application leverages modern AI tools like **Mistral LLM** for text generation and **HuggingFace embeddings** for vector search.

---

## üöÄ Features

- üìÑ **PDF Text Extraction**: Upload a PDF and extract its text content using PyPDF2.
- üß† **AI-Generated Summary**: Generate a concise summary of the document using the Mistral LLM.
- üîç **Semantic Search**: Store document chunks in a FAISS vector store with HuggingFace embeddings for efficient retrieval.
- ‚ùì **Question Answering**: Ask questions based on the document, and get answers generated by the Mistral LLM using relevant document chunks.
- üì• **Downloadable Summary**: Download the AI-generated summary as a `.txt` file.
- üñ•Ô∏è **Interactive UI**: Built with Streamlit for a user-friendly web interface.

![image](https://github.com/user-attachments/assets/4f493646-06fd-4677-9d18-3a26d7145abe)

![image](https://github.com/user-attachments/assets/c9f4287a-f9fb-4f63-bafe-07b7ac2f4c75)

---

## üõ†Ô∏è Installation

To run this application locally, follow these steps:

### 1. Clone the Repository

```bash
git clone https://github.com/ashdyl17/ai_doc_reader
cd ai-document-reader
```

### 2. Install Dependencies

Ensure you have **Python 3.8+** installed. Then, install the required packages:

```bash
pip install streamlit faiss-cpu PyPDF2 langchain langchain-ollama langchain-huggingface langchain-community numpy
```

### 3. Set Up Ollama

Install Ollama on your system:

```bash
brew install ollama         # for macOS, or follow https://ollama.com
```

Pull the Mistral model:

```bash
ollama pull mistral
```

---

## ‚ñ∂Ô∏è Run the Application

Start the Streamlit app with:

```bash
streamlit run app.py
```

The app will open in your default web browser at [http://localhost:8501](http://localhost:8501).

---

## üßë‚Äçüíª Usage

### 1. Upload a PDF
- Use the file uploader to select a PDF document.
- The app extracts the text, stores it in the FAISS vector store, and generates a summary.

### 2. View the Summary
- The AI-generated summary appears below the uploader.
- Download the summary as a `.txt` file using the "Download Summary" button.

### 3. Ask Questions
- Enter a question in the text input field.
- The app retrieves relevant document chunks from FAISS and generates an answer using the Mistral LLM.

---

## üìå Example

1. Upload a PDF containing a research paper.
2. View the summary to get a quick overview.
3. Ask specific questions like:
   - "What is the main conclusion of the paper?"
   - "What methods were used?"

---

## üß© Code Explanation

### üì¶ Dependencies

```python
import streamlit as st
import faiss
import numpy as np
import PyPDF2
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
```

### üîß Key Functions

- `extract_from_pdf(uploaded_file)`: Extracts text from a PDF using PyPDF2.
- `generate_summary(text)`: Uses the Mistral LLM to summarize the first 3000 characters of the document.
- `store_in_faiss(text, filename)`: Splits text into chunks, generates embeddings with HuggingFace, and stores them in a FAISS index.
- `retrieve_and_answer(query)`: Converts the query to an embedding, searches FAISS for relevant chunks, and generates an answer using the LLM.
- `download_summary()`: Provides a button to download the AI-generated summary as a text file.

---

## üñ•Ô∏è Streamlit UI

The user interface includes:

- File uploader for PDF input.
- Text display for summaries and answers.
- Text input for user questions.
- Download button for the summary.

---

## ‚ö†Ô∏è Limitations

- **Model Size**: The Mistral LLM and HuggingFace embeddings require significant memory and compute resources.
- **PDF Compatibility**: Some PDFs with complex formatting may not extract text accurately.
- **Summary Length**: Summaries are based on the first 3000 characters of the document, which may miss later content.
- **FAISS Index**: The index is stored in memory and reset on app restart. For persistent storage, consider saving the index to disk.

---

## üîÆ Future Improvements

- Support for multiple file uploads.
- Persistent FAISS storage using disk-based indexing.
- Allow users to select different LLMs or embedding models.
- Enhanced PDF parsing for complex documents.
- Advanced search options (e.g., filtering by document section or keyword).

---

## üß∞ Troubleshooting

| Issue                        | Solution                                                                 |
|-----------------------------|--------------------------------------------------------------------------|
| Ollama Not Running          | Ensure the Ollama service is active and the Mistral model is pulled.     |
| FAISS Errors                | Verify that `faiss-cpu` is installed correctly.                          |
| Memory Issues               | Reduce the chunk size in `store_in_faiss` or use a smaller embedding model. |
| Streamlit Not Loading       | Check that port 8501 is free and run `streamlit run app.py` again.       |

---


## üßë‚Äçüíª Built With ‚ù§Ô∏è

By Ashley Dylan
